{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ch02 강화학습 개념.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMngRmkZA7/4NskVe2JJz5z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/qhfks27/PyTorch/blob/master/Ch02_%EA%B0%95%ED%99%94%ED%95%99%EC%8A%B5_%EA%B0%9C%EB%85%90.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxK_AfleOYuQ",
        "colab_type": "text"
      },
      "source": [
        "# Ch02. 강화학습 개념\n",
        "###강화학습 개요\n",
        "강화학습 : 원하는 목표를 달성하기 위해 시간 순서대로 시스템에 가해지는 행동(action)을 선택하기 위한 방법, 머신러닝의 한 분야\n",
        "\n",
        "의사결정자 - 에이전트(agent) / 시스템 - 에이전트의 환경(environment)\n",
        "\n",
        "에이전트(agent)는 환경의 변화를 표현하는 상태(state)를 관측(observation)하여 일정한 정책(policy)하에 불연속적인 값이나 연속적인 값으로 된 행동을 선택, 결정하여 환경을 변화시킨다. 그 결과로 의사 결정 성과를 평가하는 보상(reward)을 제공받는다.\n",
        "\n",
        "강화학습은 수학적 모델을 요구하지 않고 시스템에서 얻은 데이터만 사용한다. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyBIqqc0R1oC",
        "colab_type": "text"
      },
      "source": [
        "### 마르코프 결정 프로세스(MDP, Markov decision process)\n",
        "![image](https://user-images.githubusercontent.com/52433160/82744289-5fc8f880-9db1-11ea-9bcf-a8af08be1348.png)\n",
        "\n",
        "![image](https://user-images.githubusercontent.com/52433160/82744296-740cf580-9db1-11ea-8d4a-d786aa55b061.png)\n",
        "\n",
        "$p(x_{t+1}|x_t, u_t)$  \n",
        "$\\pi(u_t|x_t) = p(u_t|x_t)$  \n",
        "$\\tau = (x_0, u_0, x_1, u_1, x_2, u_2, ..., x_T, u_T)$  \n",
        "   \n",
        "반환값(discounted return) : 시간스텝 t이후 미래에 얻을 수 있는 보상의 총합  \n",
        "감가율(discount factor, $\\gamma$) : 감가율의 값이 작을수록 에이전트가 먼 미래에 받을 보상보다는 가까운 미래에 받을 보상에 더 큰 가중치를 둔다. 0에서 1사이의 값 \n",
        "$$G_t = r(x_t, u_t)+\\gamma r(x_{t+1}, u_{t+1})+\\gamma^2 r(x_{t+2}, u_{t+2})+...+\\gamma^{T-t} r(x_T, u_T) = \t\\sum_{k=t}^T \\gamma^{k-t} r(x_k, u_k)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXGJNvxjV9JJ",
        "colab_type": "text"
      },
      "source": [
        "### 가치함수\n",
        "* 상태가치(state-value) : 어떤 상태변수 $x_t$ 에서 시작해 그로부터 정책$\\pi$에 의해서 행동이 가해졌을 때 기대할 수 있는 반환값  \n",
        "$$V^\\pi(x_t)=E_{\\tau_{u_t:u_T ~ p(\\tau_{u_t:u_T}|x_t)}}[\\sum_{k=t}^T \\gamma^{k-t} r(x_k, u_k)|x_t]  \n",
        "= \\int_{\\tau_{u_t:u_T}}^{} \\sum_{k=t}^T \\gamma^{k-t} r(x_k, u_k)\\, dx\\tau_{u_t:u_T}$$  \n",
        "상태가치는 상태변수 $x_t$ 에서 정책$\\pi$로 기대할 수 있는 미래 보상의 총합이다.  \n",
        "![image](https://user-images.githubusercontent.com/52433160/82744298-8129e480-9db1-11ea-8b1e-a2fe899510e4.png)\n",
        "\n",
        "\n",
        "* 행동가치(action-value) : 어떤 상태변수 $x_t$ 에서 행동$u_t$f를 선택하고 그로부터 정책$\\pi$에 의해서 행동이 가해졌을 때 기대할 수 있는 미래의 반환값의 기댓값\n",
        "$$Q^\\pi(x_t, u_t)=E_{\\tau_{u_t:u_T ~ p(\\tau_{u_t:u_T}|x_t)}}[\\sum_{k=t}^T \\gamma^{k-t} r(x_k, u_k)|x_t,u_t]  \n",
        "= \\int_{\\tau_{x_{t+1}:u_T}} ( \\sum_{k=t}^T \\gamma^{k-t} r(x_k, u_k))p(\\tau_{{x_{t+1}:u_T}} |x_{t}, u_t)\\, d\\tau_{x_{t+1}:u_T}$$  \n",
        "\n",
        "![image](https://user-images.githubusercontent.com/52433160/82744302-9141c400-9db1-11ea-928f-f5e7e176693e.png)\n",
        "\n",
        "* 상태가치와 행동가치와의 관계\n",
        "$$p(\\tau_{u_t:u_T}|x_t) = p(u_t, \\tau_{u_{t+1}:u_T}|x_t) = p(\\tau_{x_{t+1}:u_T}|x_t, u_t)\\pi(u_t,x_t)$$\n",
        "   \n",
        "$$V^\\pi(x_t)=\\int_{\\tau_{u_t:u_T}}^{} (\\sum_{k=t}^T \\gamma^{k-t} r(x_k, u_k))p(\\tau_{u_t:u_T}|x_t)\\, dx\\tau_{u_t:u_T} = \\int_{u_t}[\\int_{\\tau_{u_t:u_T}}\\sum_{k=t}^T \\gamma^{k-t} r(x_k, u_k))p(\\tau_{{x_{t+1}:u_T}} |x_{t}, u_t)\\, dx\\tau_{x_{t+1}:u_T}]\\pi(u_t|x_t)du_t $$  \n",
        "$$V^\\pi(x_t)=\\int_{u_t} Q^\\pi(x_t,u_t)\\pi(u_t|x_t)du_t=E_{u_t~\\pi(u_t|x_t)}[Q^\\pi(x_t,u_t)]$$\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XE8yjCwfizzP",
        "colab_type": "text"
      },
      "source": [
        "### 벨만 방정식\n",
        "벨만 방정식은 현재 상태변수의 가치와 다음 시간스텝의 상태변수의 가치와의 관계를 나타내는 식이다.\n",
        "\n",
        "$$Q^\\pi(x_t, u_t) = \\int_{\\tau_{x_{t+1}:u_T}} ( \\sum_{k=t}^T \\gamma^{k-t} r(x_k, u_k))p(\\tau_{{x_{t+1}:u_T}} |x_{t}, u_t)\\, d\\tau_{x_{t+1}:u_T}$$  \n",
        "\n",
        "$$Q^\\pi(x_t, u_t) = Q_1+Q_2$$\n",
        "$$=\\int_{\\tau_{x_{t+1}:u_T}}(r_1+\\gamma r_{t+1}+...+\\gamma^{n-1}r_{t+n-1}p(\\tau_{x_{t+1}:u_T}|x_t, u_t)d\\tau_{x_{t+1}:u_T}+\\int_{\\tau_{x_{t+1}:u_T}} ( \\sum_{k=t+n}^T \\gamma^{k-t} r(x_k, u_k))p(\\tau_{{x_{t+1}:u_T}} |x_{t}, u_t)\\, d\\tau_{x_{t+1}:u_T}$$   \n",
        "\n",
        "$$Q_1=\\int_{\\tau_{x_{t+1}:x_{t+n}}}(r_t+\\gamma r_{t+1}+...+\\gamma^{n-1}r_{t+n-1}p(\\tau_{x_{t+1}:x_{t+n}}|x_t, u_t)d\\tau_{x_{t+1}:x_{t+n}}$$\n",
        "$$Q_2=\\int_{\\tau_{x_{t+1}:u_{t+n}}} \\gamma^n[\\int_{\\tau_{x_{t+1}:u_T}}(\\sum_{k=t+n}^T \\gamma^{k-t-n} r(x_k, u_k))p(\\tau_{{x_{t+1}:u_T}} |x_{t}, u_t)\\, d\\tau_{x_{t+1}:u_T}]p(\\tau_{x_{t+1}:u_T}|x_t, u_t)d\\tau_{x_{t+1}:u_T}$$\n",
        "\n",
        "$$Q^\\pi(x_t, u_t)=E_{\\tau_{u_t:u_T ~ p(\\tau_{u_t:u_T}|x_t,u_t)}}[r_1+\\gamma r_{t+1}+...\\gamma^{n-1}r_{t+n-1}+\\gamma^n V^\\pi(x_{t+n})]$$\n",
        "\n",
        "벨만 방정식(Bellman equation)\n",
        "$$V^\\pi(x_t)=E_{u_t ~\\pi(u_t|x_t)}[r(x_t,u_t)+E_{x_{t+1}~p(x_{t+1}|x_t,u_t)}[\\gamma V^\\pi(x_{t+1})]]$$\n",
        "$$Q^\\pi(x_t, u_t)=r(x_t,u_t)+E_{x_{t+1}~p(x_{t+1}|x_t,u_t)}[E_{u_{t+1}~\\pi(u_{t+1}|u_{t+1},x_{t+1})}[\\gamma Q^\\pi(x_{t+1}, u_{t+1})]]$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yd77kTGNsMV0",
        "colab_type": "text"
      },
      "source": [
        "### 벨만 최적 방정식\n",
        "모든 정책 중에서 상태가치 값을 최대로 만드는 정책을 적용했을 때의 상태가치 함수와 행동가치 함수를 각각 최적 상태가치 함수와 최적 행동가치 함수라고 한다. \n",
        "상태가치 값을 최대로 만드는 정책을 최적 정책이라고 한다. \n",
        "\n",
        "$$V^*(x_t)=\\max_\\pi V^\\pi(x_t)$$\n",
        "$$Q^*(x_t,u_t)=\\max_\\pi Q^\\pi (x_t,u_t)$$\n",
        "\n",
        "$$Q^*(x_t,u_t)=r(x_t,u_t)+E_{x_{t+1}~p(x_{t+1}|x_t,u_t)}[\\gamma V(x_{t+1})]$$\n",
        "$$V^*(x_t)=\\max_{u_t}Q^*(x_t,u_t)$$\n",
        "$$Q^*(x_t,u_t)=r(x_t,u_t)+E_{x_{t+1}~p(x_{t+1}|x_t,u_t)}[\\gamma \\max_{u_t}Q^*(x_{t+1},u_{t+1})]$$\n",
        "$$\\pi^*(u_t|x_t)=arg\\max_{u_t}  [{r(x_t, u_t)+E_{x_{t+1}~p(x_{t+1}|x_t,u_t)}[\\gamma V^*(x_{t+1})]]}$$\n",
        "$$\\pi^*(u_t|x_t)=arg\\max_{u_t} Q^*(x_t,u_t)$$\n",
        "\n",
        "상태가치함수는 행동가치 함수를 행동에 관해 평균값을 취한 것이다.\n",
        "최적 상태가치 함수는 최적 행동가치 함수의 최댓값을 취한 것이다."
      ]
    }
  ]
}